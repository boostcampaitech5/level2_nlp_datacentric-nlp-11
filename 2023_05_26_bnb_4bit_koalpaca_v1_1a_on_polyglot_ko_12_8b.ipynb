{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XIyP_0r6zuVc"
      },
      "source": [
        "# `transformers` meets `bitsandbytes` for democratzing Large Language Models (LLMs) through 4bit quantization\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/huggingface/blog/blob/main/assets/96_hf_bitsandbytes_integration/Thumbnail_blue.png?raw=true\" alt=\"drawing\" width=\"700\" class=\"center\"/>\n",
        "</center>\n",
        "\n",
        "Welcome to this notebook that goes through the recent `bitsandbytes` integration that includes the work from XXX that introduces no performance degradation 4bit quantization techniques, for democratizing LLMs inference and training.\n",
        "\n",
        "In this notebook, we will learn together how to load a large model in 4bit (`gpt-neo-x-20b`) and train it using Google Colab and PEFT library from Hugging Face ğŸ¤—.\n",
        "\n",
        "[In the general usage notebook](https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing), you can learn how to propely load a model in 4bit with all its variants. \n",
        "\n",
        "If you liked the previous work for integrating [*LLM.int8*](https://arxiv.org/abs/2208.07339), you can have a look at the [introduction blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) to lean more about that quantization method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHX-RCPdkXgP",
        "outputId": "d294f833-6f45-4014-e4ed-75ef26141a0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed May 31 18:32:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-PCIE...  Off  | 00000000:00:05.0 Off |                  Off |\n",
            "| N/A   42C    P0    38W / 250W |  13925MiB / 32510MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuXIFTFapAMI",
        "outputId": "340f0ca7-6dfd-4350-b2d2-15f983adfc66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git \n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ-5idQwzvg-"
      },
      "source": [
        "First let's load the model we are going to use - GPT-neo-x-20B! Note that the model itself is around 40GB in half precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "278070bfaaac4794a649f11306ad2947",
            "5391d6d2385549319099d3bf653c34ee",
            "ee696f00718b43989f653dd01391a87d",
            "e39e4f5284224bfdbde4359d8593d206",
            "0a91d94890634dc1bb43176347c27fec",
            "ec9d84f9337948cba5562b7e995bb2ef",
            "3473e65596d5455385269f59e6159e38",
            "416d78e5558f4019972720c8d52a410b",
            "388757e992344b06b2fc7e4573eba77d",
            "664f6a3720a446929197650580dd005d",
            "92e0594143bd4c2690dc3ddaa0170b17"
          ]
        },
        "id": "jm4FzCvfeYcK",
        "outputId": "bf40f8d2-7603-407d-fba5-6686eddd49aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset klue-tc-tsv (/opt/ml/.cache/huggingface/datasets/naver-clova-conversation___klue-tc-tsv/default/0.0.0/df1f2c6ad91377fafd8feb2c571af2fe3c490b9f2928153d7a23a4c38da20699)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7bb3a93030e34a5e85009b1a6e5195b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"naver-clova-conversation/klue-tc-tsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KUhV7x3e6Db",
        "outputId": "1fdf62ba-5269-435c-dd11-29a381ba04c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 45678\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 9107\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 10\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FbgsI9sezTJ",
        "outputId": "7df84efa-917f-4d13-e5ba-62fc1d0b3d25"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efe71afd9ff5469791a5efab22a130ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/45678 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# # data\n",
        "# data = data.map(\n",
        "#     lambda x: \n",
        "#     {'text': f\"### ëª…ë ¹ì–´: {x['instruction']}\\n\\n###ë§¥ë½: {x['input']}\\n\\n### ë‹µë³€: {x['output']}<|endoftext|>\" }\n",
        "#     if x['input'] else \n",
        "#     {'text':f\"### ëª…ë ¹ì–´: {x['instruction']}\\n\\n### ë‹µë³€: {x['output']}<|endoftext|>\"},\n",
        "# )\n",
        "# data\n",
        "label_dict = {0 : 'ITê³¼í•™',  1 : 'ê²½ì œ', 2 : 'ì‚¬íšŒ', 3 : 'ìƒí™œë¬¸í™”', 4 : 'ì„¸ê³„', 5 :'ìŠ¤í¬ì¸ ', 6 : 'ì •ì¹˜'}\n",
        "\n",
        "data = data['train'].map(\n",
        "    lambda x: {'text': f\"### ì§ˆë¬¸: {label_dict[x['label']]}\\n\\n### ë‹µë³€: {x['text']}<|endoftext|>\" }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': '### ì§ˆë¬¸: ì •ì¹˜\\n\\n### ë‹µë³€: ìœ íŠœë¸Œ ë‚´ë‹¬ 2ì¼ê¹Œì§€ í¬ë¦¬ì—ì´í„° ì§€ì› ê³µê°„ ìš´ì˜<|endoftext|>',\n",
              " 'label': 6}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0Nl5mWL0k2T",
        "outputId": "e601d45a-99f5-46ff-aecf-497ed8754c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
            "CUDA SETUP: Detected CUDA version 110\n",
            "CUDA SETUP: Required library version not found: libbitsandbytes_cuda110_nocublaslt.so. Maybe you need to compile it from source?\n",
            "CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n",
            "\n",
            "================================================ERROR=====================================\n",
            "CUDA SETUP: CUDA detection failed! Possible reasons:\n",
            "1. CUDA driver not installed\n",
            "2. CUDA not installed\n",
            "3. You have multiple conflicting CUDA libraries\n",
            "4. Required library not pre-compiled for this bitsandbytes release!\n",
            "CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.\n",
            "CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.\n",
            "================================================================================\n",
            "\n",
            "CUDA SETUP: Something unexpected happened. Please compile from source:\n",
            "git clone git@github.com:TimDettmers/bitsandbytes.git\n",
            "cd bitsandbytes\n",
            "CUDA_VERSION=110 make cuda110_nomatmul\n",
            "python setup.py install\n",
            "CUDA SETUP: Setup Failed!\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m bnb_config \u001b[39m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      6\u001b[0m     load_in_4bit\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     bnb_4bit_use_double_quant\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     bnb_4bit_quant_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnf4\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     bnb_4bit_compute_dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbfloat16\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m---> 13\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(model_id, quantization_config\u001b[39m=\u001b[39;49mbnb_config, device_map\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m0\u001b[39;49m})\n",
            "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:490\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    489\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    491\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    492\u001b[0m     )\n\u001b[1;32m    493\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    494\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    496\u001b[0m )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py:2648\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m     keep_in_fp32_modules \u001b[39m=\u001b[39m []\n\u001b[1;32m   2647\u001b[0m \u001b[39mif\u001b[39;00m load_in_8bit \u001b[39mor\u001b[39;00m load_in_4bit:\n\u001b[0;32m-> 2648\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbitsandbytes\u001b[39;00m \u001b[39mimport\u001b[39;00m get_keys_to_not_convert, replace_with_bnb_linear\n\u001b[1;32m   2650\u001b[0m     llm_int8_skip_modules \u001b[39m=\u001b[39m quantization_config\u001b[39m.\u001b[39mllm_int8_skip_modules\n\u001b[1;32m   2651\u001b[0m     load_in_8bit_fp32_cpu_offload \u001b[39m=\u001b[39m quantization_config\u001b[39m.\u001b[39mllm_int8_enable_fp32_cpu_offload\n",
            "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/utils/bitsandbytes.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimport_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m importlib_metadata, is_accelerate_available, is_bitsandbytes_available\n\u001b[1;32m     10\u001b[0m \u001b[39mif\u001b[39;00m is_bitsandbytes_available():\n\u001b[0;32m---> 11\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbnb\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/bitsandbytes/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m cuda_setup, utils, research\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     MatmulLtState,\n\u001b[1;32m      9\u001b[0m     bmm_cublas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     matmul_4bit\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcextension\u001b[39;00m \u001b[39mimport\u001b[39;00m COMPILED_WITH_CUDA\n",
            "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/bitsandbytes/research/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mautograd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_functions\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     switchback_bnb,\n\u001b[1;32m      4\u001b[0m     matmul_fp8_global,\n\u001b[1;32m      5\u001b[0m     matmul_fp8_mixed,\n\u001b[1;32m      6\u001b[0m )\n",
            "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/bitsandbytes/research/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearFP8Mixed, LinearFP8Global\n",
            "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/bitsandbytes/research/nn/modules.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m Tensor, device, dtype, nn\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mbnb\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m GlobalOptimManager\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m OutlierTracer, find_outlier_dims\n\u001b[1;32m     11\u001b[0m T \u001b[39m=\u001b[39m TypeVar(\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, bound\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtorch.nn.Module\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/bitsandbytes/optim/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcextension\u001b[39;00m \u001b[39mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madagrad\u001b[39;00m \u001b[39mimport\u001b[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madam\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam, Adam8bit, Adam32bit, PagedAdam, PagedAdam8bit, PagedAdam32bit\n",
            "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/bitsandbytes/cextension.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m     CUDASetup\u001b[39m.\u001b[39mget_instance()\u001b[39m.\u001b[39mgenerate_instructions()\n\u001b[1;32m     19\u001b[0m     CUDASetup\u001b[39m.\u001b[39mget_instance()\u001b[39m.\u001b[39mprint_log_stack()\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'''\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[39m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001b[39m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[39m    python -m bitsandbytes\u001b[39m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[39m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[39m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[39m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001b[39m\u001b[39m'''\u001b[39m)\n\u001b[1;32m     28\u001b[0m lib\u001b[39m.\u001b[39mcadam32bit_grad_fp32 \u001b[39m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001b[39;00m\n\u001b[1;32m     29\u001b[0m lib\u001b[39m.\u001b[39mget_context\u001b[39m.\u001b[39mrestype \u001b[39m=\u001b[39m ct\u001b[39m.\u001b[39mc_void_p\n",
            "\u001b[0;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"EleutherAI/polyglot-ko-12.8b\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp2gMi1ZzGET"
      },
      "source": [
        "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4TDUgDbhyhK",
        "outputId": "8aac9a8c-5211-4cb8-bd43-3b99b24c9c00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/beomi___parquet/beomi--KoAlpaca-v1.1a-1465f66eb846fd61/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-75fcdbaed5b64b04.arrow\n"
          ]
        }
      ],
      "source": [
        "data = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9EUEDAl0ss3"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkIcwsSU01EB"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ybeyl20n3dYH",
        "outputId": "327a4a63-db3d-4631-edec-23f8d08d14ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 6553600 || all params: 6608701440 || trainable%: 0.09916622894073424\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=8, \n",
        "    lora_alpha=32, \n",
        "    target_modules=[\"query_key_value\"], \n",
        "    lora_dropout=0.05, \n",
        "    bias=\"none\", \n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FCc64bfnmd3j"
      },
      "source": [
        "Let's load a common dataset, english quotes, to fine tune our model on famous quotes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6f4z8EYmcJ6",
        "outputId": "21c8ba2d-fd89-40f3-f0b6-c73772f06bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue May 30 05:21:32 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P0    30W /  70W |   9435MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_0MOtwf3zdZp"
      },
      "source": [
        "Run the cell below to run the training! For the sake of the demo, we just ran it for few steps just to showcase how to use this integration with existing tools on the HF ecosystem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jq0nX33BmfaC",
        "outputId": "769fb34e-acd0-4a56-ac87-40db98dabd78"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 47:02, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.192100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.025800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.971500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.940200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.895000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.048800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.951200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.898600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.919400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.806600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.092700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.817300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.725100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.875600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.151900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.900700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.901100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.264800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.938400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.987100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.048400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.983300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.997100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.995700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.995900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.868700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.915200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.878300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.877700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.931400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.807300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.925000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.933100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.895300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.895000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.892500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.908200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.903600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.768100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.740700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>1.843200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>2.008600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>1.993800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.751400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.783000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.817700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>1.870900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.890400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>1.891800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.907500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=1.9244559707641602, metrics={'train_runtime': 2828.6026, 'train_samples_per_second': 0.354, 'train_steps_per_second': 0.177, 'total_flos': 1.103736813379584e+16, 'train_loss': 1.9244559707641602, 'epoch': 0.05})"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "\n",
        "# needed for gpt-neo-x tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        # warmup_steps=200,\n",
        "        max_steps=500, ## ì´ˆì†Œí˜•ë§Œ í•™ìŠµ: 10 step = 20ê°œ ìƒ˜í”Œë§Œ í•™ìŠµ.\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=10,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvP11yUbhzWP",
        "outputId": "96eb0cfe-e22f-4642-9fba-796942c62540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wow\n"
          ]
        }
      ],
      "source": [
        "print(\"wow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-jauOEv9XVe"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "model.config.use_cache = True  # silence the warnings. Please re-enable for inference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4ITTiXfp-2r",
        "outputId": "e6376ea8-f293-4ca4-f9f3-13f231f5de07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1349: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1448: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[   6,    6,    6, 2438,   29, 1832, 4770,  272,   34,  224,  202, 4588,\n",
              "         4770,  272, 1382, 5674,   34, 5000, 9357,  272]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate(**tokenizer(\"### ì§ˆë¬¸: ì˜¤ëŠ˜ ë‚ ì”¨ëŠ”?\", return_tensors='pt', return_token_type_ids=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDp9W-Gmp5Mb"
      },
      "outputs": [],
      "source": [
        "def gen(x):\n",
        "    gened = model.generate(\n",
        "        **tokenizer(\n",
        "            f\"### ì§ˆë¬¸: {x}\\n\\n### ë‹µë³€:\", \n",
        "            return_tensors='pt', \n",
        "            return_token_type_ids=False\n",
        "        ), \n",
        "        max_new_tokens=256,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,\n",
        "        eos_token_id=2,\n",
        "    )\n",
        "    print(tokenizer.decode(gened[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "iIbK1GaipZd9",
        "outputId": "46537a15-3bce-4959-860a-80706a8b3693"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'### ì§ˆë¬¸: ê±´ê°•í•˜ê²Œ ì‚´ê¸° ìœ„í•œ ì„¸ ê°€ì§€ ë°©ë²•ì€?\\n\\n### ë‹µë³€: 1. ê· í˜•ìˆëŠ” ì‹ì‚¬: í¸ì‹ì„ ë©€ë¦¬í•˜ê³ , ì±„ì†Œ, ê³¼ì¼, ìƒì„ ì„ ì„­ì·¨í•˜ë©´ ê±´ê°•í•´ì ¸ìš”.\\n\\n2. ê³¼ì‹ì„ ì¤„ì¼ ê²ƒ: ê³¼ì‹í•˜ê²Œ ë˜ë©´ ìœ„ì¥ì´ í™œë°œí•˜ì—¬ì ¸ì„œ ë³‘ì´ ë‚˜ê¸° ì‰½ê²Œ ë˜ìš”.\\n3. ì ë‹¹í•œ ìŠ¤íŠ¸ë ˆìŠ¤: ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ìŒ“ì•„ë‘ëŠ” ê²ƒì€ ë‚´ ì•ˆì— ë…ì´ ìŒ“ì´ëŠ” ê²ƒì´ë¼ì„œ ì˜¤íˆë ¤ ê±´ê°•ì— ì¢‹ì§€ ì•Šì•„ìš”.\\n\\nì¦‰, ê· í˜•ìˆëŠ” ì‹ì‚¬, ì ë‹¹í•œ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ, ê³¼ì‹ì„ ì¤„ì´ë©´ ìš°ë¦¬ ëª¸ì— ì¢‹ìŠµë‹ˆë‹¤. ì´ ì„¸ ê°€ì§€ë¥¼ ìŠµê´€í™”í•˜ë©´ì„œ ê±´ê°•í•œ ì‚¶ì„ ìœ ì§€í•œë‹¤ë©´ ê±´ê°•í•˜ê²Œ ì‚´ ìˆ˜ ìˆê²Œ ë  ê²ƒì…ë‹ˆë‹¤. \\n\\nìœ„ì˜ ì„¸ ê°€ì§€ ë°©ë²•ì€ ì´ˆë“±í•™ìƒ ë•Œë¶€í„° ë°°ìš°ë˜ ê²ƒì´ë¯€ë¡œ, ì‰½ê²Œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë‘ ê±´ê°•í•˜ê²Œ ì‚¬ëŠ” ì§€ë¦„ê¸¸ì´ë¯€ë¡œ, ì´ë¥¼ ì‹¤ì²œí•˜ì—¬ ê±´ê°•í•œ ì‚¶ì„ ì‚´ ìˆ˜ ìˆë„ë¡ ë…¸ë ¥í•©ì‹œë‹¤.\\n### ë‹µë³€ì„ ì‘ì„±í•˜ëŠ”ë°, ì•½ê°„ì˜ ì‹œê°„ì´ ê±¸ë ¸ìŠµë‹ˆë‹¤. ì§ˆë¬¸ ë‚´ìš©ì—ì„œë„ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒì²˜ëŸ¼, ê°„ë‹¨í•œ ë°©ë²•ì´ì§€ë§Œ ì‹¤í–‰ì— ì˜®ê¸°ê¸°ëŠ” ì‰¬ìš´ ì¼ì´ ì•„ë‹ˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ë…¸ë ¥í•˜ë©° ì‹¤í–‰í•œë‹¤ë©´ ì¶©ë¶„íˆ ì‹¤ì²œí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ìµœëŒ€í•œ ì§€í‚¤ë„ë¡ í•©ì‹œë‹¤. '"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen('ê±´ê°•í•˜ê²Œ ì‚´ê¸° ìœ„í•œ ì„¸ ê°€ì§€ ë°©ë²•ì€?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "1YOtvI6rpuoP",
        "outputId": "949faae7-4a6b-4074-b408-6259f696c6ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'### ì§ˆë¬¸: ìŠˆì¹´ì›”ë“œê°€ ë¬´ì—‡ì¸ê°€ìš”?\\n\\n### ë‹µë³€: ìŠˆì¹´ì›”ë“œëŠ” ìœ íŠœë¸Œ ì¸ê¸° ë°©ì†¡ì¸ìœ¼ë¡œ, ìµœì •ìƒ ê²½ì œ ìœ íŠœë²„ë¡œ í‰ê°€ë°›ê³  ìˆìŠµë‹ˆë‹¤. ìŠˆì¹´ì™€ í•¨ê»˜í•˜ëŠ” ê²½ì œì˜ ëª¨ë“  ê²ƒì´ë¼ëŠ” íƒ€ì´í‹€ë¡œ ì§„í–‰ë˜ëŠ” ì´ ì±„ë„ì—ì„œëŠ” ì£¼ì‹, ê²½ì œì— ëŒ€í•œ ë‹¤ì–‘í•œ ì´ì•¼ê¸°ê°€ ì˜¬ë¼ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. ìŠˆì¹´ì™€ í•¨ê»˜í•˜ëŠ” ê²½ì œì˜ ëª¨ë“  ê²ƒì˜ ì±„ë„ ë§í¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\\nhttps://www.youtube.com/channel/UCZ6ny7P7lEIHN5bMljHyKQw ìŠˆì¹´ì›”ë“œwww.youtube.com ìœ íŠœë¸Œì—ì„œëŠ” ê²½ì œì— ê´€ì‹¬ì„ ê°€ì§€ì‹œëŠ” ê²ƒì´ ì¢‹ì§€ ì•Šì„ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê¸ˆìœµ ê²½ì œì— ê´€ì‹¬ì´ ë§ìœ¼ì‹¤ ê²ƒ ê°™ì€ë°, ìŠˆì¹´ê°€ ìš´ì˜í•˜ëŠ” ìµœì •ìƒ ê²½ì œ ìœ íŠœë¸Œ ìŠˆì¹´ì›”ë“œë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤. íŠ¹íˆ ìŠˆì¹´ì›”ë“œ ì˜ìƒì´ ì—…ë¡œë“œë˜ëŠ” ì›”ìš”ì¼ì€ ê¼­ ì¦ê²¨ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤. ì›”ìš”ì¼ì— ì—…ë¡œë“œë˜ëŠ” ìŠˆì¹´ì›”ë“œ ì˜ìƒì€ ìŠˆì¹´ì›”ë“œì˜ ëŒ€í‘œ ì½”ë„ˆì´ë©°, ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ìˆê¸° ë•Œë¬¸ì— ë„ì›€ì´ ë©ë‹ˆë‹¤. \\n\\nê·¸ë¦¬ê³  ìŠˆì¹´ì›”ë“œì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ë“¤ë„ ì¶œì—°í•˜ê³  ìˆìŠµë‹ˆë‹¤. \\n\\nìœ íŠœë¸Œì—ì„œ ê²€ìƒ‰ ê²°ê³¼'"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen('ìŠˆì¹´ì›”ë“œê°€ ë¬´ì—‡ì¸ê°€ìš”?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "oaNYJD81sL75",
        "outputId": "bc79e6f9-6144-447c-dd3e-fead9203d64d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'### ì§ˆë¬¸: ê³µê³µì„±ì„ ì¸¡ì •í•˜ê¸°ìœ„í•œ ë°©ë²•ì€?\\n\\n### ë‹µë³€: ê³µê³µì„±ì„ ì¸¡ì •í•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ ë°©ë²• ì¤‘ í•˜ë‚˜ëŠ” ê³µê³µì„± ì²™ë„ë¥¼ êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¸¡ì • ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\nâ‘  ì‚¬íšŒì„±ê³¼ ê²½ì œì  íš¨ê³¼ - ê²½ì œì  ì´ìµê³¼ ì‚¬íšŒÂ·ë¬¸í™”ì  ì´ìµì„ ëª¨ë‘ ê³ ë ¤í•˜ëŠ” ì¢…í•©ì„±ê³¼ ê¸°ì¤€ì— ë”°ë¼ íŒŒì•….\\n\\nâ‘¡ ì •ì±…ì„±ê³¼ : ê³µê³µì„±ì˜ í‰ê°€ ì²™ë„ëŠ” ê³µê³µì •ì±…ì˜ íš¨ê³¼ë‚˜ ì˜í–¥ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” í˜œíƒê³¼ ê³µê³µ ì •ì±…ì— ëŒ€í•œ ê°€ì¹˜íŒë‹¨ì´ë‚˜ ì„ í˜¸ì— ì§ì ‘ì ìœ¼ë¡œ ì—°ê³„ëœ ì •ì±…ì  ìš”êµ¬ ì‚¬í•­ì„ ê¸°ì¤€ìœ¼ë¡œ í•©ë‹ˆë‹¤. \\n\\nâ‘¢ ì°¸ì—¬ì„±ê³¼ ê³µë™ì²´ ì˜ì‹ : ê³µê³µì„±ì˜ í‰ê°€ ì²™ë„ëŠ” ì •ì±…ê³¼ ê´€ë ¨í•œ ì‹œë¯¼ë“¤ì˜ ì§ì ‘ì°¸ì—¬ì™€ ê³µë™ì²´ ì˜ì‹, ì‹œë¯¼ë“¤ì´ ëŠë¼ëŠ” ë¬¸ì œë‚˜ ìš•êµ¬, ì‹œë¯¼ì˜ ë¬¸ì œí•´ê²° ê³¼ì •ì—ì„œì˜ ê²½í—˜ì´ë‚˜ ëŠë‚Œ ë“±ì— ëŒ€í•œ ê³µë™ì²´ ì˜ì‹ì…ë‹ˆë‹¤. \\në”°ë¼ì„œ ê³µê³µì„±ì´ ë†’ì€ ì •ì±…ì¼ìˆ˜ë¡ ê²½ì œì„±ê³¼ ì‚¬íšŒÂ·ë¬¸í™”ì  ì¸¡ë©´, ì •ì±…ì  íš¨ê³¼ ë“± ì„¸ ê°€ì§€ ìš”ì†Œê°€ ëª¨ë‘ ë†’ì€ í‰ê°€ë¥¼ ë°›ê¸° ë•Œë¬¸ì— ê³µê³µì„±ì´ ë†’ì€ ì •ì±…ì„ ì„¸ìš°ë©´ ì •ì±…ì˜ íš¨ê³¼ë¥¼ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ê³µê³µì„±ê³¼ ê´€ë ¨í•œ ì •ì±…ì„ ë§Œë“¤ ë•ŒëŠ” ì‹œë¯¼ì˜ ì°¸ì—¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°œë°œí•˜ê³ , ì •ì±… í˜•ì„± ê³¼ì •ì—ì„œ ì‹œë¯¼ë“¤ì˜ ì˜ê²¬ì´ ì ê·¹ì ìœ¼ë¡œ ë°˜ì˜ë˜ì–´ì•¼ í•˜ëŠ” ê²ƒì´'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen('ê³µê³µì„±ì„ ì¸¡ì •í•˜ê¸°ìœ„í•œ ë°©ë²•ì€?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOWSgQ6RuDK3",
        "outputId": "924e3e5f-ea24-4e6c-bf49-9665e5eb804c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1448: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### ì§ˆë¬¸: ì£¼ì‹ ì‹œì¥ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ìµì„ ì–»ê¸° ìœ„í•œ ë°©ë²•ì€?\n",
            "\n",
            "### ë‹µë³€: ì£¼ì‹ ì‹œì¥ì—ì„œ ê¾¸ì¤€íˆ ë†’ì€ ìˆ˜ìµì„ ì–»ê¸° ìœ„í•´ì„œëŠ” ë¬´ì—‡ë³´ë‹¤ë„ ì¢…ëª© ì„ íƒì„ ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ì½”ìŠ¤í”¼ ì§€ìˆ˜ê°€ í•˜ë½í•˜ë©´ ìˆ˜ìµë¥  ê´€ë¦¬ê°€ í•„ìš”í•˜ë©°, ì´ë¥¼ ìœ„í•´ íˆ¬ì ìì‚° ë°°ë¶„ ë° ê°œë³„ì¢…ëª© ì„ íƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì£¼ì‹ ì‹œì¥ì—ì„œ ë†’ì€ ìˆ˜ìµì„ ì–»ê¸° ìœ„í•´ì„œëŠ” ì¢…ëª© ì„ íƒì´ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì—, ë³¸ì¸ì˜ íˆ¬ì ì„±í–¥ì„ íŒŒì•…í•˜ì—¬ ì‹ ì¤‘í•œ íˆ¬ìë¥¼ í•´ì•¼í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ì£¼ì‹ íˆ¬ìì— ëŒ€í•œ ì „ë¬¸ì ì¸ ë¶„ì„ê³¼ ê¸°ì—… ì •ë³´, íˆ¬ì ì „ëµ ë“±ì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ê°œì¸ì´ ì§ì ‘ íˆ¬ìë¥¼ í•˜ê¸° ë³´ë‹¤ëŠ” ì „ë¬¸ê°€ì˜ ë„ì›€ì„ ë°›ì•„ íˆ¬ìí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. \n",
            "\n",
            "ë§Œì•½, ì „ë¬¸ì ì¸ ë¶„ì„ì´ ì–´ë ¤ìš¸ ê²½ìš° ì¸ê³µì§€ëŠ¥ ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•˜ë©´ ê°œì¸ë„ ì‰½ê²Œ ì„±ê³µì ì¸ íˆ¬ìë¥¼ í•  ìˆ˜ ìˆìœ¼ë‹ˆ, ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ë˜í•œ, íˆ¬ìë¥¼ í•  ë•ŒëŠ” ì‹œì¥ì„ ì •í™•íˆ íŒë‹¨í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, íˆ¬ìì˜ ìœ„í—˜ì„±ì„ ë‚®ì¶”ê¸° ìœ„í•´ì„œëŠ” ì ë¦½ì‹ íˆ¬ì, ë¶„ì‚° íˆ¬ì, ë¶„í•  ë§¤ìˆ˜ ê¸°ë²• ë“±ì„ í†µí•´ íˆ¬ìí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. \n",
            "ï¿½ë”°ë¼ì„œ, ì£¼ì‹ ì‹œì¥ì—ì„œ ë†’ì€ ìˆ˜ìµì„ ì–»ê¸° ìœ„í•´ì„œëŠ” íˆ¬ì ì „ëµ, ì¢…ëª© ì„ íƒ, ìœ„í—˜ ëŒ€ë¹„, ë¶„ì‚° íˆ¬ì ë“±ì˜ ì „ëµì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì „ë¬¸ê°€ì˜ ì¡°ì–¸ì„ ë°›ì•„ íˆ¬ì\n"
          ]
        }
      ],
      "source": [
        "gen('ì£¼ì‹ ì‹œì¥ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ìµì„ ì–»ê¸° ìœ„í•œ ë°©ë²•ì€?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPOyetf75sCv",
        "outputId": "b5e98b13-e124-494c-b254-7cdb31d56571"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### ì§ˆë¬¸: í’‹ì˜µì…˜ê³¼ ì½œì˜µì…˜ì˜ ì°¨ì´, ê·¸ë¦¬ê³  ì¼ë°˜ ê°œë¯¸ íˆ¬ììê°€ ì„ íƒí•´ì•¼ í•  í¬ì§€ì…˜ì€?\n",
            "\n",
            "### ë‹µë³€: í’‹ì´ë€ ì£¼ì‹ì„ ì‚¬ëŠ” ê²ƒ(êµ¬ë§¤)ì„ ì˜ë¯¸í•˜ê³ , ì½œì´ë€ ì£¼ì‹ì„ íŒŒëŠ”(íŒë§¤) ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ê¸°ì—…ì˜ ì£¼ê°€ê°€ 10ë§Œì›ì¸ë°, ê·¸ íšŒì‚¬ì—ì„œ ì‹ ê·œ ì‚¬ì—…ì„ ì§„í–‰ ì¤‘ì´ë¼, ì•ìœ¼ë¡œ ë§¤ì¶œì´ ì¦ëŒ€ë˜ì–´ ê¸°ì—… ê°€ì¹˜ê°€ ìƒìŠ¹í•  ê²ƒìœ¼ë¡œ íŒë‹¨í•´, í•´ë‹¹ íšŒì‚¬ì˜ ì£¼ì‹ì„ ì‚¬ëŠ” ê²ƒì´ í’‹ì˜µì…˜ì…ë‹ˆë‹¤. ì´ëŠ” ì£¼ì‹ì„ ì‚¬ëŠ” ì‹œì ê³¼ í•´ë‹¹ íšŒì‚¬ì˜ ì£¼ê°€ê°€ í•˜ë½í•  ê²½ìš°ì—ëŠ” ê¸°ì—…ì˜ ì£¼ê°€ì™€ í•¨ê»˜ ì£¼ê°€ê°€ í•˜ë½í•˜ëŠ” ë¦¬ìŠ¤í¬ê°€ ìˆìŠµë‹ˆë‹¤. ë°˜ë©´, ì½œì˜µì…˜ì€ ì£¼ì‹ì„ íŒŒëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ì£¼ì‹ì„ íŒŒëŠ” ì‹œì ê³¼ í•´ë‹¹ íšŒì‚¬ì˜ ì£¼ê°€ê°€ ì¦ê°€í•´ì„œ í–¥í›„ ê¸°ì—… ì „ë§ì´ ë°ì„ ê²½ìš° ì£¼ê°€ê°€ ìƒìŠ¹í•˜ë©°, í•´ë‹¹ ì£¼ì‹ì˜ í”„ë¦¬ë¯¸ì—„ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ì•ˆì •ì ì¸ íˆ¬ìë²•ì…ë‹ˆë‹¤. ì¼ë°˜ ê°œë¯¸ íˆ¬ììë“¤ì€ ì£¼ì‹ì„ ë§¤ìˆ˜í•  ë•Œì™€ ì£¼ì‹ì„ ë§¤ë„í•  ë•Œ ë‹¤ì–‘í•œ ë¦¬ìŠ¤í¬ê°€ ì¡´ì¬í•˜ë¯€ë¡œ, ì „ë¬¸ê°€ì˜ ì¡°ì–¸ê³¼ íˆ¬ì ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ íˆ¬ì ì „ëµì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤. íˆ¬ìì— ëŒ€í•œ ìì„¸í•œ ì‚¬í•­ì€ ì•„ë˜ì—ì„œ ë¬´ë£Œë¡œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
            "\n",
            "ì½œì˜µì…˜ ê±°ë˜: ë„¤ì´ë²„ ê²€ìƒ‰ì°½ 'ì½œì˜µì…˜ ê±°ë˜' ê²€ìƒ‰ (https://stock.naver.\n"
          ]
        }
      ],
      "source": [
        "gen('í’‹ì˜µì…˜ê³¼ ì½œì˜µì…˜ì˜ ì°¨ì´, ê·¸ë¦¬ê³  ì¼ë°˜ ê°œë¯¸ íˆ¬ììê°€ ì„ íƒí•´ì•¼ í•  í¬ì§€ì…˜ì€?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr9pvznHBgIZ",
        "outputId": "c46b40a4-0dff-4402-eb14-6cc8459e7e93"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### ì§ˆë¬¸: í’‹ì˜µì…˜ ë§¤ë„ì™€ ì½œì˜µì…˜ ë§¤ìˆ˜ì˜ ì°¨ì´, ê·¸ë¦¬ê³  ì¼ë°˜ ê°œë¯¸ íˆ¬ììê°€ ì„ íƒí•´ì•¼ í•  í¬ì§€ì…˜ì€?\n",
            "\n",
            "### ë‹µë³€: í’‹ì˜¤ë²„(í’‹ì„ ë§¤ë„í•˜ê³  ê·¸ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ê°€ê²©ì´ ë‚®ì€ ì½œì„ ì‚¬ëŠ” ê²ƒ)ë¥¼ í•œë‹¤ê³  í•´ì„œ ë” ë†’ì€ ê°€ê²©ì˜ ì½œì„ ì‚°ë‹¤ëŠ” ì˜ë¯¸ëŠ” ì•„ë‹™ë‹ˆë‹¤. ë” ë‚®ì€ ê°€ê²©ì˜ ì½œì„ ì‚¬ëŠ” ê²ƒì´ ê¸°ë³¸ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í’‹ì„ ì‚¬ëŠ” ê²ƒì€ ë§Œê¸°ê¹Œì§€ ë³´ìœ í•˜ì§€ ì•Šê³  í’‹ ë§¤ë„ í¬ì§€ì…˜ì€ íŒ” ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë‹¨ê¸°ì ìœ¼ë¡œ ë‚®ì€ ê°€ê²©ì˜ ì½œì„ ë§¤ìˆ˜í•˜ëŠ” ê²ƒë³´ë‹¤ ë†’ì€ ê°€ê²©ì˜ ì½œì„ ë§¤ìˆ˜í•  ìˆ˜ ìˆëŠ” ì„ íƒì˜ ë²”ìœ„ê°€ ìˆë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì— ë¹„í•´ ì½œì˜¤ë²„(ì½œì„ ë§¤ë„í•˜ê³  ê·¸ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ê°€ê²©ì´ ë‚®ì€ í’‹ì„ ì‚¬ëŠ” ê²ƒ)ë¥¼ í•˜ë©´ ë§Œê¸°ê¹Œì§€ ë³´ìœ í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë‹¨ê¸°ì ìœ¼ë¡œ ë†’ì€ ê°€ê²©ì˜ í’‹ì„ ì‚¬ê±°ë‚˜ ì½œì„ ì‚´ ìˆ˜ëŠ” ì—†ì§€ë§Œ, ë§Œê¸°ê¹Œì§€ ì£¼ê°€ê°€ ìƒìŠ¹í•œë‹¤ë©´ í’‹ì˜¤ë²„ëŠ” ì½œì˜¤ë²„ì— ë¹„í•´ ë†’ì€ ìˆ˜ìµì„ ë‚¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë˜í•œ ì½œê³¼ í’‹ì˜ ê°€ê²©ì„ ë¹„êµí•´ ë³´ë©´ ë†’ì€ ê°€ê²©ì˜ ìª½ì´ ë†’ì€ ê°€ê²©ì— ë¹„í•´ ìˆ˜ìµì„±ì´ ë” ë†’ìœ¼ë¯€ë¡œ ì½œì„ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. ì¦‰, ì„ íƒì˜ í­ì€ ë” ë†’ì€ ê°€ê²©ì˜ ì½œì„ ì‚´ ê²ƒì¸ì§€, ì•„ë‹ˆë©´ ë” ë‚®ì€ ê°€ê²©ì˜ ì½œì„ ì‚´ ê²ƒì¸ì§€ì— ë‹¬ë ¤ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œë„ ëª…ì‹¬í•´ì•¼\n"
          ]
        }
      ],
      "source": [
        "gen('í’‹ì˜µì…˜ ë§¤ë„ì™€ ì½œì˜µì…˜ ë§¤ìˆ˜ì˜ ì°¨ì´, ê·¸ë¦¬ê³  ì¼ë°˜ ê°œë¯¸ íˆ¬ììê°€ ì„ íƒí•´ì•¼ í•  í¬ì§€ì…˜ì€?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYrc_CZuCrR6",
        "outputId": "8d21485b-df66-4b83-a782-9b91f5eb392c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### ì§ˆë¬¸: ë§ˆì§„ì½œì´ ë°œìƒí•˜ëŠ” ì´ìœ ê°€ ë­ì•¼? ê·¸ë¦¬ê³  ì–´ë–»ê²Œ í•´ì•¼ ë§ˆì§„ì½œì„ ë§‰ì„ ìˆ˜ ìˆì–´?\n",
            "\n",
            "### ë‹µë³€: ë§ˆì§„ì½œì´ë€, ì„ ë¬¼/ì˜µì…˜ ë§¤ë§¤ ì‹œ ì¦ê±°ê¸ˆì´ ë¶€ì¡±í•´ì„œ ì¶”ê°€ì¦ê±°ê¸ˆ ì˜ˆì¹˜ë¥¼ ìš”êµ¬ë°›ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ì¶”ê°€ì¦ê±°ê¸ˆì€ ì„ ë¬¼/ì˜µì…˜ ê±°ë˜ ê³„ì•½ì‹œ ë¯¸ë¦¬ ì •í•´ì§„ ì•½ì •ì¦ê±°ê¸ˆì—ì„œ ì¶”ê°€ë¡œ ë¶€ë‹´í•´ì•¼ í•˜ëŠ” ì¦ê±°ê¸ˆì„ ì˜ë¯¸í•©ë‹ˆë‹¤. í†µìƒì ìœ¼ë¡œ ë§¤ìˆ˜ ê³„ì•½ ì‹œì—ëŠ” ê³„ì•½ ê¸ˆì•¡ì˜ 10%ë¥¼ ì¦ê±°ê¸ˆìœ¼ë¡œ ì˜ˆì¹˜í•˜ê³ , ë§¤ë„ ê³„ì•½ ì‹œì—ëŠ” í•´ë‹¹ ê¸ˆì•¡ì˜ 3%ë¥¼ ì˜ˆì¹˜í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ë§¤ìˆ˜ ê³„ì•½ ì‹œì—ëŠ” ì´ 20%ì˜ ì¦ê±°ê¸ˆì´ í•„ìš”í•˜ê³ , ë§¤ë„ ê³„ì•½ ì‹œì—ëŠ” ì´ 9%ì˜ ì¦ê±°ê¸ˆì´ í•„ìš”í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì„ ë¬¼ ë§¤ë§¤ ì¤‘ì—ëŠ” ë§¤ì¼ ë§¤ì¼ì˜ ì‹œì„¸ì— ë”°ë¼ ì¦ê±°ê¸ˆì„ ìˆ˜ì‹œë¡œ í•„ìš”í•˜ë©´ ì¶”ê°€ë¡œ ë‚©ë¶€í•´ì•¼í•´ì„œ ì¶”ê°€ì¦ê±°ê¸ˆì´ ë°œìƒí•˜ë©°, í•´ë‹¹ í•„ìš”ì•¡ ë¶€ì¡±ì‹œì—ëŠ” ë§ˆì§„ì½œì´ ë°œìƒí•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ì„ ë¬¼ ê±°ë˜ë¥¼ í•˜ê¸° ì „ì—ëŠ” ê¼­ ì¶”ê°€ì¦ê±°ê¸ˆì„ ì˜ˆì¹˜í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì¶”ê°€ì¦ê±°ê¸ˆì€ 'ì„ ë¬¼/ì˜µì…˜ -> ì¦ê±°ê¸ˆ -> ì¶”ê°€ì˜ˆì¹˜/ì²­ì‚°' ë°©ë²•ìœ¼ë¡œ ì¡°íšŒ ê°€ëŠ¥í•˜ë©°, ì¸í„°ë„·ì´ë‚˜ ì „í™”ë¡œë„ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜, 'ì£¼ë¬¸/ì¦ê±°ê¸ˆ -> ì¶”ê°€ì¦ê±°ê¸ˆ'ì„ í†µí•´ ì¶”ê°€ì¦ê±°ê¸ˆì„ ì…ê¸ˆí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ì¦ê±°ê¸ˆì„ ì˜ˆì¹˜\n"
          ]
        }
      ],
      "source": [
        "gen(\"ë§ˆì§„ì½œì´ ë°œìƒí•˜ëŠ” ì´ìœ ê°€ ë­ì•¼? ê·¸ë¦¬ê³  ì–´ë–»ê²Œ í•´ì•¼ ë§ˆì§„ì½œì„ ë§‰ì„ ìˆ˜ ìˆì–´?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-5etmH-fMKj"
      },
      "outputs": [],
      "source": [
        "# ### ì§ˆë¬¸: ë§ˆì§„ì½œì´ ë°œìƒí•˜ëŠ” ì´ìœ ê°€ ë­ì•¼? ê·¸ë¦¬ê³  ì–´ë–»ê²Œ í•´ì•¼ ë§ˆì§„ì½œì„ ë§‰ì„ ìˆ˜ ìˆì–´?\n",
        "\n",
        "# ### ë‹µë³€: ë§ˆì§„ì½œì´ë€, ì„ ë¬¼/ì˜µì…˜ ë§¤ë§¤ ì‹œ ì¦ê±°ê¸ˆì´ ë¶€ì¡±í•´ì„œ ì¶”ê°€ì¦ê±°ê¸ˆ ì˜ˆì¹˜ë¥¼ ìš”êµ¬ë°›ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. \n",
        "# ì¶”ê°€ì¦ê±°ê¸ˆì€ ì„ ë¬¼/ì˜µì…˜ ê±°ë˜ ê³„ì•½ì‹œ ë¯¸ë¦¬ ì •í•´ì§„ ì•½ì •ì¦ê±°ê¸ˆì—ì„œ ì¶”ê°€ë¡œ ë¶€ë‹´í•´ì•¼ í•˜ëŠ” ì¦ê±°ê¸ˆì„ ì˜ë¯¸í•©ë‹ˆë‹¤. \n",
        "# í†µìƒì ìœ¼ë¡œ ë§¤ìˆ˜ ê³„ì•½ ì‹œì—ëŠ” ê³„ì•½ ê¸ˆì•¡ì˜ 10%ë¥¼ ì¦ê±°ê¸ˆìœ¼ë¡œ ì˜ˆì¹˜í•˜ê³ , ë§¤ë„ ê³„ì•½ ì‹œì—ëŠ” í•´ë‹¹ ê¸ˆì•¡ì˜ 3%ë¥¼ ì˜ˆì¹˜í•©ë‹ˆë‹¤. \n",
        "# ë”°ë¼ì„œ, ë§¤ìˆ˜ ê³„ì•½ ì‹œì—ëŠ” ì´ 20%ì˜ ì¦ê±°ê¸ˆì´ í•„ìš”í•˜ê³ , ë§¤ë„ ê³„ì•½ ì‹œì—ëŠ” ì´ 9%ì˜ ì¦ê±°ê¸ˆì´ í•„ìš”í•©ë‹ˆë‹¤. \n",
        "# ë”°ë¼ì„œ, ì„ ë¬¼ ë§¤ë§¤ ì¤‘ì—ëŠ” ë§¤ì¼ ë§¤ì¼ì˜ ì‹œì„¸ì— ë”°ë¼ ì¦ê±°ê¸ˆì„ ìˆ˜ì‹œë¡œ í•„ìš”í•˜ë©´ ì¶”ê°€ë¡œ ë‚©ë¶€í•´ì•¼í•´ì„œ ì¶”ê°€ì¦ê±°ê¸ˆì´ ë°œìƒí•˜ë©°, í•´ë‹¹ í•„ìš”ì•¡ ë¶€ì¡±ì‹œì—ëŠ” ë§ˆì§„ì½œì´ ë°œìƒí•©ë‹ˆë‹¤. \n",
        "# ë”°ë¼ì„œ, ì„ ë¬¼ ê±°ë˜ë¥¼ í•˜ê¸° ì „ì—ëŠ” ê¼­ ì¶”ê°€ì¦ê±°ê¸ˆì„ ì˜ˆì¹˜í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. \n",
        "# ì¶”ê°€ì¦ê±°ê¸ˆì€ 'ì„ ë¬¼/ì˜µì…˜ -> ì¦ê±°ê¸ˆ -> ì¶”ê°€ì˜ˆì¹˜/ì²­ì‚°' ë°©ë²•ìœ¼ë¡œ ì¡°íšŒ ê°€ëŠ¥í•˜ë©°, ì¸í„°ë„·ì´ë‚˜ ì „í™”ë¡œë„ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
        "# ë˜, 'ì£¼ë¬¸/ì¦ê±°ê¸ˆ -> ì¶”ê°€ì¦ê±°ê¸ˆ'ì„ í†µí•´ ì¶”ê°€ì¦ê±°ê¸ˆì„ ì…ê¸ˆí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ì¦ê±°ê¸ˆì„ ì˜ˆì¹˜"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a91d94890634dc1bb43176347c27fec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "278070bfaaac4794a649f11306ad2947": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5391d6d2385549319099d3bf653c34ee",
              "IPY_MODEL_ee696f00718b43989f653dd01391a87d",
              "IPY_MODEL_e39e4f5284224bfdbde4359d8593d206"
            ],
            "layout": "IPY_MODEL_0a91d94890634dc1bb43176347c27fec"
          }
        },
        "3473e65596d5455385269f59e6159e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "388757e992344b06b2fc7e4573eba77d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "416d78e5558f4019972720c8d52a410b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5391d6d2385549319099d3bf653c34ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec9d84f9337948cba5562b7e995bb2ef",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3473e65596d5455385269f59e6159e38",
            "value": "100%"
          }
        },
        "664f6a3720a446929197650580dd005d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92e0594143bd4c2690dc3ddaa0170b17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e39e4f5284224bfdbde4359d8593d206": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_664f6a3720a446929197650580dd005d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_92e0594143bd4c2690dc3ddaa0170b17",
            "value": " 1/1 [00:00&lt;00:00,  9.41it/s]"
          }
        },
        "ec9d84f9337948cba5562b7e995bb2ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee696f00718b43989f653dd01391a87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_416d78e5558f4019972720c8d52a410b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_388757e992344b06b2fc7e4573eba77d",
            "value": 1
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
